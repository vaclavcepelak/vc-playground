{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "pytorch_mps_high_watermark_ratio = os.environ.get(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\")\n",
    "if pytorch_mps_high_watermark_ratio is not None:\n",
    "    print(f\"PYTORCH_MPS_HIGH_WATERMARK_RATIO = {pytorch_mps_high_watermark_ratio}\")\n",
    "else:\n",
    "    print(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, PreTrainedTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset via Kaggle API\n",
    "# https://towardsdatascience.com/how-to-search-and-download-data-using-kaggle-api-f815f7b98080\n",
    "# os.system(\n",
    "#     \"kaggle datasets download -d rounakbanik/the-movies-dataset -p ./data/ --unzip\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/movies_metadata.csv\", usecols=[\"overview\", \"genres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genres\"] = df[\"genres\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"genres\"].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df.explode(\"genres\")\n",
    "df_exp = pd.concat([df_exp[[\"overview\"]], df_exp[\"genres\"].apply(pd.Series)], axis=1).reset_index(names=[\"movie_id\"]).reset_index(names=[\"input_id\"])\n",
    "# df_exp = df_exp.groupby([\"movie_id\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre_cnts = df_exp.groupby([\"id\", \"name\"]).size().to_frame(\"cnt\").query(\"cnt > 1\").reset_index().sort_values(by=\"id\")\n",
    "df_genre_cnts[\"label\"] = df_genre_cnts.reset_index().index\n",
    "df_exp = df_exp.merge(df_genre_cnts[[\"id\", \"label\"]], on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_train, mov_test = train_test_split(df_exp[\"movie_id\"], test_size=0.2, stratify=df_exp[\"id\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_rename = {\"overview\": \"text\"}\n",
    "train_data = df_exp.loc[df_exp[\"movie_id\"].isin(mov_train), [\"label\", \"overview\"]].rename(columns=dct_rename)#.to_dict(orient=\"dict\")\n",
    "test_data = df_exp.loc[df_exp[\"movie_id\"].isin(mov_test), [\"label\", \"overview\"]].rename(columns=dct_rename)#.to_dict(orient=\"dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.sample(20000)\n",
    "test_data_sample = test_data.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_label, fast_tokenizer=True)\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     result = tokenizer(\n",
    "#         [str(txt) for txt in examples[\"text\"]], \n",
    "#         padding=\"max_length\", \n",
    "#         truncation=True, \n",
    "#         max_length=512, \n",
    "#         return_overflowing_tokens=True\n",
    "#     )\n",
    "    \n",
    "#     sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "#     for key, values in examples.items():\n",
    "#         result[key] = np.array([values[i] for i in sample_map])\n",
    "#     return result\n",
    "# Creating a function for tokenization\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer([str(txt) for txt in examples[\"text\"]], truncation=True, max_length=512, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_pandas(train_data_sample)\n",
    "# test_dataset = Dataset.from_pandas(test_data_sample)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized.to_pandas().input_ids.apply(len).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_label, num_labels=df_genre_cnts.shape[0])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    # weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = train_data[\"label\"].value_counts(normalize=True).sort_index()\n",
    "class_weights = (pct.mean() / pct).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute metrics\n",
    "# # Source: https://medium.com/cometheartbeat/building-a-text-classifier-app-with-hugging-face-bert-and-comet-278e4cd0d0aa (Step 6)\n",
    "# # Indexing to example function\n",
    "# # def get_example(index):\n",
    "# #   return test_dataset_tokenized[index][\"text\"]\n",
    "\n",
    "# compute custom loss (suppose one has 3 labels with different weights)\n",
    "def weighted_cross_entropy_loss(labels, logits, class_weights):\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, class_weights.shape[0]), labels.view(-1))\n",
    "        return loss\n",
    "\n",
    "# # Creating a function to compute metrics\n",
    "# def compute_metrics(pred):\n",
    "#     # experiment = comet_ml.get_global_experiment()\n",
    "\n",
    "#     labels = pred.label_ids\n",
    "#     logits = pred.predictions\n",
    "#     preds = logits.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "#         labels, preds, average=\"macro\"\n",
    "#     )\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     # cross_entropy = weighted_cross_entropy_loss(labels=labels, logits=logits, class_weights=class_weights)\n",
    "\n",
    "#     # if experiment:\n",
    "#     #     epoch = int(experiment.curr_epoch) if experiment.curr_epoch is not None else 0\n",
    "#     #     experiment.set_epoch(epoch)\n",
    "#     #     experiment.log_confusion_matrix(\n",
    "#     #         y_true=labels,\n",
    "#     #         y_predicted=preds,\n",
    "#     #         file_name=f\"confusion-matrix-epoch-{epoch}.json\",\n",
    "#     #         labels=[\"negative\", \"positive\"],\n",
    "#     #         index_to_example_function=get_example,\n",
    "#     #     )\n",
    "\n",
    "#     return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomTrainer for imbalanced dataset\n",
    "# Source: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss = weighted_cross_entropy_loss(labels=labels, logits=logits, class_weights=class_weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems with memory here:\n",
    "# https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/9133\n",
    "# solution: restart the notebook everytime after it crashes?\n",
    "# model = model.to(\"mps\")\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=test_dataset_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "test_predictions = trainer.predict(test_dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my (correct) solution:\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "preds = softmax(test_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds.argmax(1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=pd.Series(preds.argmax(axis=1)), columns=test_data_sample.label.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=pd.Series(preds.argmax(axis=1)), columns=test_data_sample.label.reset_index(drop=True), normalize=0, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_movie_tokenized = tokenizer(\n",
    "#     test_movie, \n",
    "#     padding=\"max_length\", \n",
    "#     truncation=True,   \n",
    "#     max_length=512, \n",
    "#     return_overflowing_tokens=True\n",
    "# )\n",
    "def get_prediction(test_movie):\n",
    "    test_movie_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": [str(test_movie)], \"label\": [0]}))\n",
    "    test_movie_tokenized = test_movie_dataset.map(tokenize_function, batched=True)\n",
    "    outputs = trainer.predict(test_movie_tokenized)\n",
    "    probs = softmax(outputs.predictions)\n",
    "    return df_genre_cnts[[\"label\", \"name\"]].assign(probs=probs[0]).sort_values(by=\"probs\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_movie = \"A teenage boy with a sex therapist mother teams up with a high school classmate to set up an underground sex therapy clinic at school.\"\n",
    "test_movie = \"An orphaned boy enrolls in a school of wizardry, where he learns the truth about himself, his family and the terrible evil that haunts the magical world.\"\n",
    "# test_movie = \"After uncovering a mysterious artifact buried beneath the Lunar surface, a spacecraft is sent to Jupiter to find its origins: a spacecraft manned by two men and the supercomputer HAL 9000.\"\n",
    "get_prediction(test_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
